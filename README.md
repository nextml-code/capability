# Introduction
The purpose of this repository is to prove our capability to deliver machine learning solutions. It's short and to the point.

In machine learning, simple solutions usually work, and "cool" tricks redundant. Our goal is to deliver value, not to "impress" with overly complicated solutions.

With that said, we still want to prove that we have the skills (both in machine learning and programming) to develop and implement cutting-edge algorithms.

Here, you can read about some of our projects and solutions. We can't share the entire codebases, but we will add links to relevant code-snippets.

### Technologies

We use PyTorch or Tensorflow to develop algorithms, but we prefer PyTorch. In the experimentation phase, we use [guildai](https://github.com/guildai/guildai) to keep track of every experiment. Guildai gives us many tools to compare different runs, and we can easily select the best algorithm.

To make sure that you can use the algorithm in your systems, we create a lightweight and easy to deploy microservice.

# Projects

### Railroad Inspection

In collaboration with two companies working with railroad maintenance, we are developing algorithms for identifying critical infrastructure and anomalies.

One algorithm is responsible for detecting clamps on contact wires. It's a simple object detection problem, but there are some challenges:

- The dataset is very unbalanced. 99% of the images has no clamps, and there are different types of clamps
- The algorithm has to analyze a massive amount of data, so speed is critical√§
- There are multiple cameras
- The images are overlapping

To handle the unbalanced dataset we sample images based on class-distribution, and then sample again based on loss. The algorithm has an [interesting architecture](https://github.com/Aiwizo/capability/blob/master/railroad_inspection/architecture.py) with multiple outputs.


### Object Tracking

To measure the flow of trafic in intersections we developed algorithms for tracking objects across frames. It uses [correlation filters](https://github.com/Aiwizo/capability/blob/master/object_tracking/correlation.py) for auto-correlation and cross-correlation in order to generalize better to new data.

There was a lack of annotated data in the domain of interest which provided an interesting challenge. We trained the first algorithm on made up data that we generated using random noise.

### Audio Denoising

Currently, we are building a product for audio denoising. The training data is generated by combining voice audio clips from random pocasts with audio clips containing different types of noise. We make sure that the [signal-to-noise ratio](https://en.wikipedia.org/wiki/Signal-to-noise_ratio) is close to 0dB and we use STFT to convert the waveforms to spectograms.

The algorithm is has a UNet-architecture with EfficientNet as its backbone. It predicts a smooth mask with values between 0-1 that we multiple with the spectogram of the mixed audio.

A snapshot of our data pipeline can be found [here](https://github.com/Aiwizo/capability/tree/master/audio_denoising/data.py)!

### Legal

One customer spend thosands of hours every year to find risk in legal documents. We let the legal department annotate risk in real contracts and developed algorithms to make them more effective.

The algorithm in production is a transformer, which is the current state of the art in NLP. The results became better we used BERT, but the improved performance did not outdo the loss in speed.

### Lably

For annotating data we have created a novel tool called Lably. The reasoning behind Lably is that, in a large datasets, some datapoints are more important than other. In Lably, the algorithm performing the task is also responsible for choosing the next data point to annotate.


# Recreational projects

### Semi-supervised

With semi-supervised learning we can create useful algorithms with very little data. Initially, we did [mixup and mixmatch on mnist with 10 annotated examples](https://github.com/Aiwizo/mnist). The results was good right away, with an accuracy score of 78%. 

The implementation and usage is relatively simple:
- [mixup (tensorflow)](https://github.com/Aiwizo/capability/blob/master/semi_supervised/mixup.py)
- [mixmatch (tensorflow, extension of mixup)](https://github.com/Aiwizo/capability/blob/master/semi_supervised/mixmatch.py)
- [mixmatch (pytorch)]([mixmatch-pytorch](https://github.com/FelixAbrahamsson/mixmatch-pytorch)


### Generating climbing problems
Two of our colleagues are passionate about bouldering. They developed an algorithm that creates problems on a climbing board. The chosen holds are strongly dependent on each other and was modelled in a few different ways:

- Decoupled sampling by predicting the next hold using modified loss
- Predict full board using the [Gumbel-softmax trick](https://pytorch.org/docs/stable/distributions.html#relaxedonehotcategorical) and modified loss for steps in-between
- Hybrid variantional autoencoder with adverserial loss
- Variational autoencoder
- Generative adverserial network

Features like difficulty were also introduced to the model to steer what kind of problem would be created.


### Discrete relaxation
Much like the [Gumbel-softmax trick](https://pytorch.org/docs/stable/distributions.html#relaxedonehotcategorical) that tries to let us get gradients through a discrete transformation, there is the idea that we can replace the derivative of a discrete function during training and analyze what happens mathematically. This has already shown to be very useful in [sequential modelling](https://arxiv.org/pdf/1801.09797.pdf) and creating better [variational autoencoders](https://arxiv.org/pdf/1906.00446.pdf). We implemented a couple of versions of our own and [the original](https://github.com/Aiwizo/capability/blob/master/kaiser_step.py) to try on some simple problems.


### Exploration by uncertainty estimation in world model
Attempt at using Bayesian model uncertainty for chosing paths for exploration. We had used variational inference a number of times and exploration seems like a natural place for its use. The [repo is called labyrinth and can be found here](https://github.com/samedii/labyrinth). We are using classic VI to learn the world model and using the uncertainty as a reward when learning an exploring agent. It gave ok results and could solve the labyrinth but the world model was a little weak. It might give better results using the [Kaiser step](https://github.com/Aiwizo/capability/blob/master/kaiser_step.py) that came out since to encode the latent state of the world.
