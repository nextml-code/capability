# Introduction
The purpose of this repository is to prove our capability to deliver machine learning solutions. It’s short and to the point.

In machine learning, simple solutions usually work, and “cool” tricks often redundant. Our goal is to deliver value, not to “impress” with overly complicated solutions.

With that said, we still want to prove that we have the skills (both in machine learning and programming) to develop and implement cutting-edge algorithms.

Here, you can read about some of our projects and solutions. We can’t share the entire codebases, but there are links to relevant code-snippets.

### Technologies

We use PyTorch or Tensorflow to develop algorithms, but we prefer PyTorch. In the experimentation phase, we use [guildai](https://github.com/guildai/guildai) to keep track of each experiment. Guildai gives us many tools to compare runs and select the best algorithm.

To make sure that you can use the algorithm in your systems, we create a lightweight and easy to deploy microservice.

# Projects

### Railroad Inspection

In collaboration with two companies working with railroad maintenance, we are developing algorithms for identifying critical infrastructure and anomalies in images taken with cameras mounted on trains.

One algorithm is responsible for detecting clamps on contact wires. It’s a simple object detection problem, but there are some challenges:

- 99% of the images have no clamps, and the class distribution is also unbalanced.
- The algorithm has to analyze a massive amount of data, so the speed is critical.
- There are multiple cameras, and the images are overlapping

To handle the unbalanced dataset, we sample images based on class-distribution and then sample again based on loss. The algorithm has an [interesting architecture](https://github.com/Aiwizo/capability/blob/master/railroad_inspection/architecture.py) to make it fast and to support multiple outputs.

### Object Tracking

To measure the flow of traffic in intersections, we developed algorithms for tracking objects across frames. It uses [correlation filters](https://github.com/Aiwizo/capability/blob/master/object_tracking/correlation.py) for auto-correlation and cross-correlation in order to generalize better to new data.

There was a lack of annotated data in the domain of interest, which provided an exciting challenge. We trained the first algorithm on synthetic that we generated by placing random noise on different textures.

To make the algorithm work better for the domain, we placed the noise on backgrounds extracted from the videos.

### Audio Denoising

Currently, we are building a product for audio denoising. The training data is generated by combining audio clips containing voices from random podcasts with audio clips containing different types of noise. We make sure that the [signal-to-noise ratio](https://en.wikipedia.org/wiki/Signal-to-noise_ratio) is close to 0dB, and we use STFT to convert the waveforms to spectrograms.

The algorithm is a UNet with EfficientNet as its backbone. It predicts a smooth mask with values between 0-1 that we multiple with the spectrogram of the mixed audio.

You can find a snapshot of our data pipeline [here](https://github.com/Aiwizo/capability/tree/master/audio_denoising/data.py)!

### Identifying Legal Risk

One customer spends thousands of hours every year to find risk in legal documents. We let the legal department annotate risk in contracts and trained algorithms on that data.

The algorithm in production is a transformer, which is the current state of the art in NLP. The results became better when we used BERT, but the improved performance did not outdo the loss in speed.

We implemented several creative augmentations to the training data:
- Replacing words with similar words based on word vectors from fasttext.
- Removed words at random from sentences.

### Faster Annotation

For annotating data, we have created a novel tool called Lably. The reasoning behind Lably is that, in large datasets, some data points are more important than others. 

In Lably, the algorithm performing the task is also responsible for choosing the next data point to annotate.

# Recreational projects

### Semi-supervised

With semi-supervised learning, we can create useful algorithms with very little data. Initially, we did mixup and mixmatch on mnist with ten annotated examples. The results were good right away, with an accuracy score of 78%. The implementation and usage are relatively simple:
- [mixup (tensorflow)](https://github.com/Aiwizo/capability/blob/master/semi_supervised/mixup.py)
- [mixmatch (tensorflow, extension of mixup)](https://github.com/Aiwizo/capability/blob/master/semi_supervised/mixmatch.py)
- [mixmatch (pytorch)]([mixmatch-pytorch](https://github.com/FelixAbrahamsson/mixmatch-pytorch)

### Generating climbing problems
The team at Aiwizo goes bouldering every Thursday. For fun, we developed an algorithm that generates problems on a climbing board. The chosen holds are strongly dependent on each other and were modeled in a few different ways:

- Decoupled sampling by predicting the next hold using modified loss
- Predict full board using the [Gumbel-softmax trick](https://pytorch.org/docs/stable/distributions.html#relaxedonehotcategorical) and modified loss for steps in-between
- Hybrid variational autoencoder with adversarial loss
- Variational autoencoder
- Generative adversarial network

### Discrete relaxation
Much like the [Gumbel-softmax trick](https://pytorch.org/docs/stable/distributions.html#relaxedonehotcategorical) that tries to let us get gradients through a discrete transformation, there is the idea that we can replace the derivative of a discrete function during training and analyze what happens mathematically. This has already shown to be very useful in [sequential modeling](https://arxiv.org/pdf/1801.09797.pdf) and creating better [variational autoencoders](https://arxiv.org/pdf/1906.00446.pdf). We implemented a couple of versions of our own and [the original](https://github.com/Aiwizo/capability/blob/master/kaiser_step.py) to try on some simple problems.

### Exploration by uncertainty estimation in world model
Attempt at using Bayesian model uncertainty for choosing paths for exploration. We had used variational inference several times, and exploration seems like a natural place for its use. The [repo is called labyrinth and can be found here](https://github.com/samedii/labyrinth). We are using classic VI to learn the world model and using the uncertainty as a reward when learning an exploring agent. It gave ok results and could solve the labyrinth, but the world model was a little weak. It might provide better results using the [Kaiser step](https://github.com/Aiwizo/capability/blob/master/kaiser_step.py) that came out since to encode the latent state of the world.
